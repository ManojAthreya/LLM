{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6xh7KK_e5l9"
      },
      "source": [
        "### Information Retreival - LangChain\n",
        "- Without Using OpenAI Embeddings\n",
        "- Without OpenAI LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HbUlsp3fS5p"
      },
      "source": [
        "Two Applications:\n",
        "- Text Documents\n",
        "- Multiple PDF Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEZmjGQGW1ue",
        "outputId": "6ffa840e-31e9-4efb-f413-38298f109ed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.306)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.41)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.34.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.27.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (17.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.14.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.16.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install huggingface_hub\n",
        "!pip install sentence_transformers\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra7uy0RhXd_2"
      },
      "source": [
        "### Get HUGGINGFACEHUB_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AZVtuZiW9Il"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_XXXXXXXXXXXXXXXXXXXXXXXXX\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_hOAJzQmQi6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hALLW9fyXWqu"
      },
      "source": [
        "### Download Text File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9lr1BOMXTVr"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt\"\n",
        "res = requests.get(url)\n",
        "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
        "  f.write(res.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbVWiGvZ-qyv"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/Hallucinations.pdf\")\n",
        "documents = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVio4mK5XVwR"
      },
      "outputs": [],
      "source": [
        "# Document Loader\n",
        "#from langchain.document_loaders import TextLoader\n",
        "#loader = TextLoader('./state_of_the_union.txt')\n",
        "#documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du41v2SOnXtG",
        "outputId": "2c01640d-14da-4181-a5a5-eceece786ef3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Challenges in Domain-Speciﬁc Abstractive Summarization and How to\\nOvercome Them\\nAnum Afzal1, Juraj Vladika1, Daniel Braun2and Florian Matthes1\\n1Department of Computer Science, Technical University of Munich, Boltzmannstrasse 3, 85748\\nGarching bei Muenchen, Germany\\n2Department of High-tech Business and Entrepreneurship, University of Twente, Hallenweg 17,\\n \\nKeywords: Text Summarization, Natural Language Processing, Efﬁcient Transformers, Model Hallucination, Natural\\nLanguage Generation Evaluation, Domain-Adaptation of Language Models.\\nAbstract: Large Language Models work quite well with general-purpose data and many tasks in Natural Language\\nProcessing. However, they show several limitations when used for a task such as domain-speciﬁc abstractive\\ntext summarization. This paper identiﬁes three of those limitations as research problems in the context of\\nabstractive text summarization: 1) Quadratic complexity of transformer-based models with respect to the\\ninput text length; 2) Model Hallucination, which is a model’s ability to generate factually incorrect text; and\\n3) Domain Shift, which happens when the distribution of the model’s training and test corpus is not the same.\\nAlong with a discussion of the open research questions, this paper also provides an assessment of existing\\nstate-of-the-art techniques relevant to domain-speciﬁc text summarization to address the research gaps.\\n1 INTRODUCTION\\nWith the ever-increasing amount of textual data be-\\ning created, stored, and digitized, companies and re-\\nsearchers have large corpora at their disposal that\\ncould be processed into useful information. Perusal\\nand encapsulation of such data usually require domain\\nexpertise which is costly and time-consuming. Ab-\\nstractive text summarization using Natural Language\\nProcessing (NLP) techniques, is a powerful tool that\\ncan provide aid for this task. Unlike the traditional\\nautomatic text summarization techniques, which ex-\\ntracts the most relevant sentences from the original\\ndocument, abstractive text summarization generates\\nnew text as summaries. For the sake of simplicity, the\\nterm text summarization would be used to represent\\nabstractive text summarization in this paper.\\nWhile text summarization (Gupta and Gupta,\\n2019; Klymenko et al., 2020) on general textual data\\nhas been an active research ﬁeld in the past decade,\\nsummarization of domain-speciﬁc documents, espe-\\ncially to support business and scientiﬁc processes\\nhave not received much attention. State-of-the-art\\nresearch focuses on deep learning models in NLP\\nto capture semantics and context associated with the\\ntext. While these Large Language Models (LLMs)perform well on the general-purpose corpus, their\\nperformance declines when tested against domain-\\nspeciﬁc corpus. This paper discusses some challenges\\nLLMs face in the context of a text summarization task\\nand provides an overview of existing techniques that\\ncould be leveraged to counter those challenges.\\nPrevious research in text summarization has\\nmostly focused on general-purpose data (Gupta and\\nGupta, 2019; Allahyari et al., 2017). Domain-speciﬁc\\nsummarization however, is still an active research area\\nand has many research questions that need to be ad-\\ndressed. This paper addresses some of those theo-\\nretical research questions and provides an initial as-\\nsessment of the existing techniques can be utilized\\nto overcome those limitations. We have identiﬁed\\nthree important hurdles with regards to the success-\\nful implementation of an NLP model for generation\\nof domain-speciﬁc summaries.\\n1. Most language models have quadratic complex-\\nity, meaning that the memory requirement grows\\nquadratically as the length of the text increases.\\nAs a result, transformer-based language models\\nare not capable of processing large documents.\\nSince most documents that need to be summarized\\nare long, it creates a need for language models ca-\\npable of handling them efﬁciently without over-682\\nAfzal, A., Vladika, J., Braun, D. and Matthes, F .', metadata={'source': '/content/Hallucinations.pdf', 'page': 0}),\n",
              " Document(page_content='are long, it creates a need for language models ca-\\npable of handling them efﬁciently without over-682\\nAfzal, A., Vladika, J., Braun, D. and Matthes, F .\\nChallenges in Domain-Speciﬁc Abstractive Summarization and How to Overcome Them.\\nDOI: 10.5220/0011744500003393\\nInProceedings of the 15th International Conference on Agents and Artiﬁcial Intelligence (ICAART 2023) - Volume 3 , pages 682-689\\nISBN: 978-989-758-623-1; ISSN: 2184-433X\\nCopyright c⃝2023 by SCITEPRESS – Science and Technology Publications, Lda. Under CC license (CC BY -NC-ND 4.0)', metadata={'source': '/content/Hallucinations.pdf', 'page': 0}),\n",
              " Document(page_content='shooting in terms of complexity.\\n2. Evaluating generated summaries is difﬁcult us-\\ning common evaluation metrics, that look at word\\noverlaps between generated summaries and the\\nreference text. This curbs model expressiveness\\nin favor of repeating the original human wording.\\nGenerated summaries can include information not\\npresent in the original document, a phenomenon\\nknown as model hallucination. Factually incor-\\nrect summaries are problematic in domains like\\nscience or journalism because they can produce\\nmisinformation and reduce the trust in models.\\n3. State-of-the-art text summarization models are\\npre-trained on general-purpose corpus and hence\\ndo not perform well on domain-speciﬁc text. This\\nhappens because a domain-speciﬁc text contains\\nwords and concepts that were not a part of the\\noriginal model training vocabulary. When gen-\\nerating summaries, it is essential for the model\\nto encode the text properly, which is usually not\\nthe case since the model fails to capture domain-\\nspeciﬁc concepts.\\nHence, to produce concise and meaningful domain-\\nspeciﬁc summaries, it is important to address the fol-\\nlowing three research gaps:\\n• How to overcome the input size limitation of\\ntransformer-based language model so they can\\nprocess large documents without running into\\ncomplexity issues?\\n• How to evaluate summaries generated by a lan-\\nguage model to ensure they convey all the impor-\\ntant information while being factually correct?\\n• How can we adapt an existing general-purpose\\nlanguage model to understand the underlying con-\\ncepts and vocabulary of the new domain?\\nThis paper is divided into ﬁve sections. The ﬁrst sec-\\ntion provided an introduction to the topic and out-\\nlined three important hurdles faced in domain-speciﬁc\\nsummarization. Section 2 builds up on the research\\ngaps and further elaborates them. Section 3 outlines\\nthe existing techniques that can be used to overcome\\nthose research gaps, followed by Section 4 that ini-\\ntiates a comparative discussion on the existing tech-\\nniques. Finally, Section 5 concludes this paper and\\nprovides hints related to the future work.\\n2 CURRENT CHALLENGES IN\\nTEXT SUMMARIZATION\\nFor a task such as text summarization, a sequence-\\nto-sequence (Seq2Seq) architecture that takes text asinput and produces text as output, is the most suit-\\nable one. Since traditional seq2seq models like Re-\\ncurrent Neural Networks (RNNs) and Long short-\\nterm memory (LSTMs) (Hochreiter et al., 1997) have\\nsome inherent limitations, such as not being able to\\nencode long-term dependencies in text and lack of\\nparallelism opportunities, they are not suitable for\\ndomain-speciﬁc summarization of long documents.\\nTransformer-based seq2seq models address these lim-\\nitations by allowing computations to be parallelized,\\nretain long-term dependencies via a self-attention ma-\\ntrix, and better text encoding through a word embed-\\nding module that has been trained on a huge corpus.\\nAs discussed in the section below, these models come\\nwith their own set of impediments when utilized for\\nsummarization of domain-speciﬁc long documents.\\n2.1 Transformers and Their Quadratic\\nComplexity\\nFirst introduced in the paper Attention is all you need\\n(Vaswani et al., 2017), the Transformers immediately\\nbecame popular and have since been the backbone of\\nLLMs. By design, they are pre-trained on a huge cor-\\npus allowing them to learn the patterns, context, and\\nother linguistic aspects of the text. Furthermore, they\\nare trained using self-supervised approaches that al-\\nlow them to make use of the huge corpora of unstruc-\\ntured and unlabeled data. The heart of a Transformer\\nblock however, is the self-attention matrix that helps\\nit retain the long-term context of the text. The self-\\nattention matrix essentially tells the model how much\\nattention a word should pay to all the other words in\\nthe text. While this information is vital, its calcula-\\ntion consumes a huge amount of memory and takes\\na long time to compute. The calculation of the n×n', metadata={'source': '/content/Hallucinations.pdf', 'page': 1}),\n",
              " Document(page_content='the text. While this information is vital, its calcula-\\ntion consumes a huge amount of memory and takes\\na long time to compute. The calculation of the n×n\\nself-attention matrix, where nis the number of tokens\\n(sequence length), entails quadratic complexity.As a\\nworkaround, the input text is typically truncated to re-\\ntain only the ﬁrst 512 tokens. For tasks such as text\\nsummarization, it is important for the model to en-\\ncodes the entire input text and hence, this problem is\\nstill an open research area.\\n2.2 NLG Evaluation and Hallucinations\\nA common challenge in generating summaries from\\nscratch is how to meaningfully evaluate their content\\nand ensure factual consistency with the source text.\\n2.2.1 Evaluating Summarization\\nNatural Language Generation (NLG) is a subset of\\nNLP dealing with tasks where new text is generated,Challenges in Domain-Speciﬁc Abstractive Summarization and How to Overcome Them\\n683', metadata={'source': '/content/Hallucinations.pdf', 'page': 1}),\n",
              " Document(page_content='one of them being abstractive summarization. The\\noutput of models for NLG tasks is notoriously hard\\nto evaluate because there is usually a trade-off be-\\ntween the expressiveness of the model and its fac-\\ntual accuracy (Sai et al., 2022). Metrics to evaluate\\ngenerated text can be word-based, character-based, or\\nembedding-based. Word-based metrics are the most\\npopular evaluation metrics, owing to their ease of use.\\nThey look at the exact overlap of n-grams (n consec-\\nutive words) between generated and reference text.\\nTheir main drawback is that they do not take into ac-\\ncount the meaning of the text. Two sentences such\\nas “Berlin is the capital of Germany ” and “ Berlin is\\nnot the capital of Germany ” have an almost complete\\nn-gram overlap despite having opposite meanings.\\n2.2.2 Model Hallucinations\\nEven though modern transformer models can gener-\\nate text that is coherent and grammatically correct,\\nthey are prone to generating content not backed by the\\nsource document. Borrowing the terminology from\\npsychology, this is called model hallucination. In ab-\\nstractive summarization, the summary is said to be\\nhallucinated if it has any spans not supported by con-\\ntent in the input document (Maynez et al., 2020). Two\\nmain types of hallucinations are (1) intrinsic, where\\nthe generated content contradicts the source docu-\\nment; and (2) extrinsic, which are facts that cannot be\\nveriﬁed from the source document. For example, if\\nthe document mentions an earthquake that happened\\nin 2020, an intrinsic hallucination is saying it hap-\\npened in 2015, while an extrinsic one would be a sen-\\ntence about a ﬂood that is never even mentioned in\\nthe document. In their analysis of three recent state-\\nof-the-art abstractive summarization systems, (Falke\\net al., 2019) show that 25% of generated summaries\\ncontain hallucinated content. Hallucinations usually\\nstem from pre-trained large models introducing facts\\nthey learned during their training process, which is\\nunrelated to a given source document.\\n2.3 Domain Shift in Natural Language\\nProcessing\\nWhen working with speciﬁc NLP applications, do-\\nmain knowledge is paramount for success. Finding\\nlabeled training data, or even unlabeled data in some\\ncases, is a big challenge. Training data is often scarce\\nin many domains/languages and often hinders the so-\\nlution development for domain-speciﬁc tasks in NLP.\\nTransfer Learning provides a solution to this by uti-\\nlizing the existing model knowledge and building on\\nit when training the model for a new task. Essentially,it allows the transfer and adaptation of the knowledge\\nacquired from one set of domains and tasks to another\\nset of domains and tasks.\\nTransformer-based language models in tandem\\nwith Transfer Learning have proven to be quite suc-\\ncessful in the past years and have found their appli-\\ncation in several real-world use cases. While they\\nwork well with tasks involving general-purpose cor-\\npus, there is a performance decline when it comes\\nto domain-speciﬁc data. This happens because these\\nlanguage models are pre-trained on general-purpose\\ndata but are then tested on a domain-speciﬁc corpus.\\nThis difference in the distribution of training and test-\\ning data is known as the Domain Shift problem in\\nNLP. It essentially means that the model doesn’t know\\nthe domain-speciﬁc corpus contains words and con-\\ncepts since they were not part of model’s pre-training.\\n3 EXISTING TECHNIQUES\\nThis section presents an overview of the existing tech-\\nniques and architectures that can be applied for the\\nsummarization of domain-speciﬁc documents. These\\ntechniques are categorized into three sections based\\non the research questions they address; Efﬁcient\\nTransformers, Evaluation metrics, and Domain adap-\\ntation of Language Models. These techniques are\\nsummarized in Table 1, and discussed in detail in the\\nsection below.\\n3.1 Efﬁcient Transformers\\nThe quadratic complexity of the Transformer block\\nis a well-known issue and several approaches to', metadata={'source': '/content/Hallucinations.pdf', 'page': 2}),\n",
              " Document(page_content='summarized in Table 1, and discussed in detail in the\\nsection below.\\n3.1 Efﬁcient Transformers\\nThe quadratic complexity of the Transformer block\\nis a well-known issue and several approaches to\\ncounter this have been proposed in the past years.\\nAll of these approaches focusing on adapting the self-\\nattention mechanism of the Transformer block to re-\\nduce the quadratic complexity are categorized as Efﬁ-\\ncient Transformers. The survey by Tay et al. provides\\na detailed taxonomy of all available Efﬁcient Trans-\\nformers (Tay et al., 2020). Some state-of-the-art Ef-\\nﬁcient Transformers suitable for domain-speciﬁc text\\nsummarization are discussed below:\\nBigBird. BigBird is a long sequence Transformer that\\nwas introduced by Zaheer et al. and can process up to\\n4,096 tokens at a time. The attention mechanism of\\nBigBird essentially consists of three parts in which\\nall tokens attend to 1) a set of global tokens, 2) a set\\nof randomly chosen tokens, and 3) all tokens in di-\\nrect adjacency (Zaheer et al., 2020). The set of global\\ntokens attending to the entire sequence consists of ar-\\ntiﬁcially introduced tokens. The local attention is im-\\nplemented in form of a sliding window of a prede-ICAART 2023 - 15th International Conference on Agents and Artiﬁcial Intelligence\\n684', metadata={'source': '/content/Hallucinations.pdf', 'page': 2}),\n",
              " Document(page_content='Table 1: An overview of the research gaps, the proposed solutions, and the existing techniques that can be utilized for domain-\\nspeciﬁc abstractive summarization as discussed in Sections 2 and 3.\\nChallenges Proposed Solution Existing Techniques\\nQuadratic Complexity of\\nTransformer ModelsEfﬁcient TransformersBigBird\\nLongformer Encoder-Decoder\\nReformer, Performers\\nNLG Evaluation and\\nHallucination MitigationSemantic Evaluation Metrics\\nFact-CheckingMETEOR, BERTScore\\nNLI-based, QA-based\\nDomain shift in\\nLanguage ModelsDomain-adaptation of\\nLanguage ModelsFine-tuning-based\\nPre-training-based\\nTokenization-based\\nﬁned width w, in which a token attends to the w/2\\npreceding and following tokens in the sequence. The\\nBigBird model’s memory complexity is linear with\\nregard to the length of the input sequence, i.e., it is\\nO(N)(Tay et al., 2020).\\nLongformer Encoder-Decoder. The Longformer\\nEncoder-Decoder (LED) model is a variant of the\\nLongformer for sequence-to-sequence tasks such as\\nsummarization or translation (Beltagy et al., 2020).\\nSimilar to the BigBird model, the original Long-\\nformer relies on a sliding window attention of width\\nwwith each token attending to the w/2 preceding and\\nfollowing tokens in the sequence. Stacking multiple\\nlayers, each using sliding window attention, ensures\\nthat a large amount of contextual information is em-\\nbedded in each token’s encoding. Apart from sliding\\nwindow attention, the authors also use dilated slid-\\ning window attention. This in effect reduces the res-\\nolution of the sequence and allows the model to in-\\nclude more contextual information with ﬁxed com-\\nputational costs. The Longformer model also incor-\\nporates global attention. Similar to BigBird’s global\\nattention, a set of predeﬁned positions in the input se-\\nquence attend to the entire sequence and all tokens in\\nthe sequence attend to the same global tokens. LED\\nhas an encoder that uses the local+global attention\\npattern of the original Longformer and a decoder that\\nuses the full self-attention on the encoding provided\\nby the encoder. The LED model scales linearly as the\\ninput length increases and hence has a complexity of\\nO(N)(Tay et al., 2020).\\nReformer The Reformer (Kitaev et al., 2020) fol-\\nlows a two-step approach to reduce the complexity of\\nthe Transformer block. Firstly, the Reformer model\\nmakes use of reversible residual networks RevNet\\n(Gomez et al., 2017) which allow the model to store\\nonly one instance of the activations rather than hav-\\ning to store activations for every layer to be able to\\nuse back-propagation. In RevNets any layer’s acti-\\nvations can be restored from the ones of the follow-\\ning layer and the model’s parameters (Gomez et al.,\\n2017) hence reducing the model’s memory require-ments drastically. Secondly, to reduce the quadratic\\ncomplexity with regard to the input sequence’s length,\\nthe authors use locality-sensitive hashing to approxi-\\nmate the attention matrix. The attention mechanism’s\\noutsized memory requirements result from the com-\\nputation of the attention matrix, i.e., so ftmax (QKT√dk),\\nand in that mainly the computation of QKT. The\\nauthors point out that applying the softmax function\\nimplies that the attention matrix is dominated by the\\nlargest elements of QKT. These largest elements re-\\nsult from the dot-product of the query and key vec-\\ntors that are most similar to each other. Kitaev et al.\\nnote that the attention matrix can, consequently, be\\nefﬁciently approximated by only computing the dot-\\nproduct of those query and key vectors with the clos-\\nest distance to each other. The Reformer uses locality-\\nsensitive hashing to determine the closest neighbors\\nof each query vector. The memory complexity of the\\nLSH attention mechanism is O(NlogN)in the length\\nof the input sequence (Tay et al., 2020).\\nPerformers . The Performer architecture relies on\\na mechanism known as Fast Attention Via positive\\nOrthogonal Random features (FA VOR+) to approxi-\\nmate the self-attention matrix in kernel space. This', metadata={'source': '/content/Hallucinations.pdf', 'page': 3}),\n",
              " Document(page_content='a mechanism known as Fast Attention Via positive\\nOrthogonal Random features (FA VOR+) to approxi-\\nmate the self-attention matrix in kernel space. This\\ntechnique is different from the previously discussed\\nones since it does not make any assumptions about\\nthe behavior of the self-attention matrix such as\\nlow-rankness or sparsity and guarantees low estima-\\ntion variance, uniform convergence, and an almost-\\nunbiased estimation of the original self-attention ma-\\ntrix. The authors further state that the Performer is\\ncompatible with existing pre-trained language mod-\\nels and requires little further ﬁne-tuning (Choroman-\\nski et al., 2020). The Performer’s complexity is O(N)\\n(Tay et al., 2020) in terms of time and space.\\n3.2 Semantic Evaluation Metrics and\\nFact-Checking of Hallucinations\\nNumerous metrics have been devised for evaluating\\ngenerated summaries. Word-based metrics look at n-\\ngram overlaps between a candidate summary and theChallenges in Domain-Speciﬁc Abstractive Summarization and How to Overcome Them\\n685', metadata={'source': '/content/Hallucinations.pdf', 'page': 3}),\n",
              " Document(page_content='source document, while semantic evaluation metrics\\ntake into account the meaning of generated words and\\nsentences. Many state-of-the-art generative models\\nfor summarization produce hallucinations, so there is\\nan increasing effort to detect and eliminate them.\\n3.2.1 Evaluation Metrics\\nWord-Based Metrics. These metrics look at exact\\noverlap between words in candidate summaries and\\ngold summary. BLEU is a metric based on precision\\nwhich computes the n-gram overlap between the gen-\\nerated and the reference text (Papineni et al., 2002).\\nIt is calculated for different values of nand for all\\ngenerated candidate summaries that are to be evalu-\\nated. The ﬁnal BLEU-N score is the geometric mean\\nof all intermediate scores for all values of n. ROUGE\\nis a metric similar to BLEU, but it is based on re-\\ncall instead of precision (Lin, 2004). This means that\\nfor any given n, it counts the total number of n-grams\\nacross all the reference summaries, and ﬁnds out how\\nmany of them are present in the candidate summary.\\nSemantic Evaluation Metrics. Since both BLEU\\nand ROUGE look at exact word matching, this leaves\\nno room for synonyms or paraphrases. METEOR is\\na metric (Banerjee and Lavie, 2005) that builds up\\non BLEU by relaxing the matching criteria. It takes\\ninto account word stems and synonyms, so that two\\nn-grams can be matched even if they are not exactly\\nthe same. Moving away from synonym matching,\\nembedding-based metrics capture the semantic sim-\\nilarity by using dense word/sentence embeddings, to-\\ngether with vector-based similarity measures (like co-\\nsine similarity), to evaluate how closely the summary\\nmatches the source text. BERTScore is one such met-\\nric that utilizes BERT-based contextual embeddings\\nof generated text and reference text in order to calcu-\\nlate the similarity between them (Zhang et al., 2020).\\n3.2.2 Hallucination Detection\\nDetecting hallucinations in generated summaries is\\nstill a challenging task, for which dedicated methods\\nare developed. Based on the availability of annotated\\ntraining data, these approaches can be split into unsu-\\npervised and semi-supervised (Huang et al., 2021).\\nUnsupervised Metrics . These metrics rely on repur-\\nposing approaches for other NLP tasks like informa-\\ntion extraction (IE), natural language inference (NLI),\\nor question answering (QA) for the task of hallucina-\\ntion detection. The motivation behind this is the avail-\\nability of training datasets for these tasks as opposed\\nto scarce datasets for hallucination detection. The IE-\\nbased metrics compare the sets of extracted triples\\n(subject, relation, object) and named entities fromboth the source document and generated summary\\nto detect hallucination (Goodrich et al., 2019). The\\nNLI-based approaches in try to determine whether the\\ngenerated summary logically entails the source docu-\\nment with a high probability (Falke et al., 2019). The\\nQA-based approaches work by posing the same set of\\nquestions to both the original document and the gener-\\nated summary, and then comparing the two sets of ob-\\ntained answers. Intuitively, a non-hallucinated sum-\\nmary and the source document will provide similar\\nanswers to the posed questions (Gabriel et al., 2021).\\nSemi-Supervised Metrics. This type of metric re-\\nlies on datasets designed speciﬁcally for the task of\\nhallucination detection. The data is usually syntheti-\\ncally generated from existing summarization datasets.\\nFor example, the weakly-supervised model FactCC\\n(Kryscinski et al., 2020) was trained jointly on three\\ntasks: sentence factual consistency, supporting evi-\\ndence extraction from source, and incorrect span de-\\ntection in generated summaries. Similarly, in (Zhou\\net al., 2021) a transformer model was trained on syn-\\nthetic data with inserted hallucinations with the task\\nof predicting hallucinated spans in summaries.\\n3.2.3 Hallucination Mitigation\\nThe approaches to mitigate hallucinations in summa-\\nrization can generally be divided into pre-processing', metadata={'source': '/content/Hallucinations.pdf', 'page': 4}),\n",
              " Document(page_content='of predicting hallucinated spans in summaries.\\n3.2.3 Hallucination Mitigation\\nThe approaches to mitigate hallucinations in summa-\\nrization can generally be divided into pre-processing\\nmethods, that try to modify the model architecture\\nor training process so that models can generate more\\nfactually-aware summaries in the ﬁrst place, and post-\\nprocessing methods, that aim to correct hallucinations\\nin already generated candidate summaries.\\nPre-Processing Methods. The main line of work\\nhere focuses on augmenting the model architecture\\nby modifying the encoder or decoder component\\nof sequence-to-sequence models. One way of en-\\nhancing the encoders is injecting external knowledge\\ninto them before the training process, such as world\\nknowledge triples from Wikidata (Gunel et al., 2019).\\nFor improving the decoding process, tree-based de-\\ncoders were used (Song et al., 2020a). Another line of\\nresearch involves modifying the training process. For\\nexample, contrastive learning was used in (Cao and\\nWang, 2021), where positive examples were human-\\nwritten summaries and negative examples were hallu-\\ncinatory, generated summaries.\\nPost-Processing Methods. These methods approach\\nthe problem by detecting the incorrect facts in the\\nﬁrst version of a generated summary and then cor-\\nrecting them for the ﬁnal version. For this purpose, in\\n(Chen et al., 2021) contrast candidate generation was\\nused to replace incorrect named entities in summaries\\nwith those entities present in the source document.ICAART 2023 - 15th International Conference on Agents and Artiﬁcial Intelligence\\n686', metadata={'source': '/content/Hallucinations.pdf', 'page': 4}),\n",
              " Document(page_content='One promising research direction that has not been\\nexplored a lot is applying methods of fact-checking\\nfor hallucination detection and correction. Such an\\napproach was used in (Dziri et al., 2021), where re-\\nsponses of conversational agents were checked and\\nfactually corrected before being sent out to users. The\\ntask of automated fact-checking consists of assessing\\nthe veracity of factual claims based on evidence from\\nexternal knowledge sources (Zeng et al., 2021). It is\\nusually performed in a pipeline fashion, where ﬁrst\\nrelevant documents and sentences are retrieved as ev-\\nidence, and then veracity is predicted by inferring if\\nthere is entailment between the claim and evidence.\\nRecently, there is an increasing interest in automat-\\nically verifying claims related to science, medicine,\\nand public health (Kotonya and Toni, 2020).\\n3.3 Domain Adaptation of Language\\nModels\\nDomain adaptation of Language Models has been a\\nhot research area recently giving rise to several ap-\\nproaches. Some approaches relevant to abstractive\\ntext summarization are discussed below:\\nFine-Tuning-Based. The most commonly used ap-\\nproach involves ﬁne-tuning a pre-trained language\\nmodel on a smaller task-speciﬁc dataset. In general,\\nﬁne-tuning means retraining an existing model to ad-\\njust its weights to the speciﬁc-domain dataset or task\\nso the model can make better predictions. One such\\napproach is portrayed by Karouzos et al. where they\\nﬁrst employ continued training on a BERT architec-\\nture utilizing a Masked Language Model loss. This\\napproach is different from standard ﬁne-tuning ap-\\nproaches because it makes use of an unlabeled corpus\\nfor domain adaptation. As a second step, they ﬁne-\\ntune the domain-adapted model from the previous\\nstep on a classiﬁcation task (Karouzos et al., 2021).\\nPre-Training-Based. A pre-training-based approach\\nas compared to a ﬁne-tuning-based approach trains\\nthe model weights from scratch instead of contin-\\nued training on previously trained weights. In the\\npast years, there have been many research contribu-\\ntions in the area of text summarization but it has been\\nmostly restricted to general-purpose corpus. One\\nsimilar approach involving a pre-training-based ap-\\nproach is presented by the authors Moradi et al.\\nwhere they utilize a combination of graph-based and\\nembedding-based approaches for the extractive sum-\\nmarization of biomedical article (Moradi et al., 2020).\\nTo counter the domain shift problem, they ﬁrst re-\\ntrain a BERT architecture on medical documents to\\nensure the availability of domain-speciﬁc word em-bedding. Then they generate sentence-level embed-\\nding of the input documents using the previously re-\\ntrained model. To generate summaries, they employ\\na graph-based approach to assign weights to previ-\\nously generated sentence-level embedding and fol-\\nlowed a sentence ranking algorithm to select the can-\\ndidates for the summary generation. Another simi-\\nlar approach related to multi-domain adaptive models\\nis presented by Zhong et al. for a text summariza-\\ntion task. They use the existing BART(Lewis et al.,\\n2019) architecture and exploit the multitask learning\\nobjective (including text summarization, text recon-\\nstruction, and text classiﬁcation) to expand the cov-\\nerage area of the existing model without changing its\\narchitecture (Zhong et al., 2022).\\nTokenization-Based. A tokenization-based approach\\ninvolves updating the model tokenizer (Song et al.,\\n2020b; Kudo and Richardson, 2018) to either in-\\nclude new domain-speciﬁc words or inﬂuencing its\\nalgorithm to prioritize a sequence of sub-words be-\\nlonging to the domain-speciﬁc corpus. While ﬁne-\\ntuning and pre-training is a basic yet powerful tech-\\nnique for domain adaptation, over the years, some\\nauthors have contributed to this problem by employ-\\ning tokenization-based techniques. Sachidananda et\\nal. for instance propose an alternate approach where\\nthey adapt the RoBERTa (Liu et al., 2019) tokenizer\\nto include words from the domain-speciﬁc corpus.', metadata={'source': '/content/Hallucinations.pdf', 'page': 5}),\n",
              " Document(page_content='al. for instance propose an alternate approach where\\nthey adapt the RoBERTa (Liu et al., 2019) tokenizer\\nto include words from the domain-speciﬁc corpus.\\nMost tokenization schemes typically merge subwords\\nto create an individual token if that combination has\\na higher frequency in the domain-speciﬁc corpus.\\nSachidananda et al. approach this by inﬂuencing the\\ntokenizer to prioritize such subword sequences from\\nthe domain-speciﬁc corpus rather than the base cor-\\npus (Sachidananda et al., 2021).\\n4 DISCUSSION\\nWhile the end goal of all Efﬁcient Transformers is to\\nreduce the quadratic complexity of the self-attention\\nmatrix, the techniques employed by them can be cat-\\negorized into 1) techniques that assume sparsity of\\nthe self-attention matrix and compute only a few rel-\\nevant entries, or 2) techniques that take advantage of\\nmathematical compositions of the self-attention ma-\\ntrix such as Low Rankness, transformation to a Ker-\\nnel Space, and other optimizations to reduce the com-\\nplexity. In general, all efﬁcient transformers have per-\\nformance close to the original transformer on bench-\\nmark datasets but their performance in the real-life ap-\\nplication is yet to be evaluated.\\nEffectively evaluating generated summaries is an on-\\ngoing challenge. Recent embedding-based metricsChallenges in Domain-Speciﬁc Abstractive Summarization and How to Overcome Them\\n687', metadata={'source': '/content/Hallucinations.pdf', 'page': 5}),\n",
              " Document(page_content='such as BERTScore take into account the context and\\nsemantics of sentences and are better correlated with\\nhuman judgment. Still, these metrics are way more\\ncomputationally intensive, their score is dependent on\\nthe PLM used, and they lack the intuitive explainabil-\\nity that standard scores like BLEU or ROGUE pro-\\nvide. There are domains, such as legislative, where\\nspeciﬁc terms and sentence structure is important to\\nbe preserved in the summary, therefore classic word-\\nbased metrics are preferred for evaluating them.\\nTo overcome the domain shift in LLMs, several tech-\\nniques have been proposed by researchers. When\\nworking with LLMs, the availability of task-speciﬁc\\ntraining data is a challenge. In most cases, the deci-\\nsion between ﬁne-tuning or pre-training can be based\\non the availability of the training resources and data.\\nIf enough domain-speciﬁc training data and comput-\\ning resources are available, pre-training a domain-\\nspeciﬁc model might always be the best choice. A\\ntokenization-based approach can be used exclusively\\nwith a ﬁne-tuning-based approach as an additional\\nadd-on to enhance performance.\\n5 CONCLUSION AND FUTURE\\nWORK\\nWe assume that domain-speciﬁc text summarization\\nwill gain importance in the research ﬁeld of NLP due\\nto its ability to automate the task of manual summa-\\nrization. This paper is meant to serve as a founda-\\ntion step for research along the three research gaps ad-\\ndressed. While there are several promising NLP mod-\\nels for abstractive text summarization (Zhang et al.,\\n2019; Lewis et al., 2019), they are not efﬁcient in\\ntheir training techniques as the size of the input doc-\\numents increases. Moreover, when tested on the\\ndomain-speciﬁc corpus, they suffer from the domain-\\nshift problem and often hallucinate because they were\\ntrained on general-purpose corpora and lack domain\\nknowledge. On top of that, the automatic evaluation\\nof the generated text is still a challenge. To the best of\\nour knowledge, there have been several contributions\\nto each of these individual research gaps however, an\\nintegrated approach addressing them from a text sum-\\nmarization perspective is lacking. A domain-adapted\\nefﬁcient transformer architecture in tandem with ex-\\nternal fact-checking mechanisms and better automatic\\nevaluation metrics could drastically improve the per-\\nformance of text summarization models. The future\\nwork could be contributions towards the individual re-\\nsearch gaps with the end goal of an integrated solution\\nfor text summarization.REFERENCES\\nAllahyari, M., Pouriyeh, S., Asseﬁ, M., Safaei, S., Trippe,\\nE. D., Gutierrez, J. B., and Kochut, K. (2017). Text\\nsummarization techniques: A brief survey.\\nBanerjee, S. and Lavie, A. (2005). METEOR: An automatic\\nmetric for MT evaluation with improved correlation\\nwith human judgments. In Proceedings of the ACL\\nWorkshop on Intrinsic and Extrinsic Evaluation Mea-\\nsures for Machine Translation and/or Summarization ,\\npages 65–72, Ann Arbor, Michigan. Association for\\nComputational Linguistics.\\nBeltagy, I., Peters, M. E., and Cohan, A. (2020). Long-\\nformer: The long-document transformer. arXiv\\npreprint arXiv:2004.05150 .\\nCao, S. and Wang, L. (2021). CLIFF: Contrastive learning\\nfor improving faithfulness and factuality in abstractive\\nsummarization. In Proceedings of the 2021 Confer-\\nence on Empirical Methods in Natural Language Pro-\\ncessing , Online and Punta Cana, Dominican Republic.\\nAssociation for Computational Linguistics.\\nChen, S., Zhang, F., Sone, K., and Roth, D. (2021). Improv-\\ning faithfulness in abstractive summarization with\\ncontrast candidate generation and selection. In Pro-\\nceedings of the 2021 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies , Online.\\nAssociation for Computational Linguistics.\\nChoromanski, K., Likhosherstov, V ., Dohan, D., Song, X.,\\nGane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiud-\\ndin, A., Kaiser, L., et al. (2020). Rethinking attention', metadata={'source': '/content/Hallucinations.pdf', 'page': 6}),\n",
              " Document(page_content='Choromanski, K., Likhosherstov, V ., Dohan, D., Song, X.,\\nGane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiud-\\ndin, A., Kaiser, L., et al. (2020). Rethinking attention\\nwith performers. arXiv preprint arXiv:2009.14794 .\\nDziri, N., Madotto, A., Za ¨ıane, O., and Bose, A. J. (2021).\\nNeural path hunter: Reducing hallucination in dia-\\nlogue systems via path grounding. In Proceedings of\\nthe 2021 Conference on Empirical Methods in Nat-\\nural Language Processing , pages 2197–2214, Online\\nand Punta Cana, Dominican Republic. Association for\\nComputational Linguistics.\\nFalke, T., Ribeiro, L. F. R., Utama, P. A., Dagan, I., and\\nGurevych, I. (2019). Ranking generated summaries by\\ncorrectness: An interesting but challenging applica-\\ntion for natural language inference. In Proceedings of\\nthe 57th Annual Meeting of the Association for Com-\\nputational Linguistics , Florence, Italy. Association for\\nComputational Linguistics.\\nGabriel, S., Celikyilmaz, A., Jha, R., Choi, Y ., and Gao,\\nJ. (2021). GO FIGURE: A meta evaluation of fac-\\ntuality in summarization. In Findings of the Asso-\\nciation for Computational Linguistics: ACL-IJCNLP\\n2021 , pages 478–487, Online. Association for Com-\\nputational Linguistics.\\nGomez, A. N., Ren, M., Urtasun, R., and Grosse, R. B.\\n(2017). The reversible residual network: backpropa-\\ngation without storing activations. Proceedings of the\\n31st International Conference on Neural Information\\nProcessing Systems , pages 2211–2221.\\nGoodrich, B., Rao, V ., Liu, P. J., and Saleh, M. (2019).\\nAssessing the factual accuracy of generated text. In\\nProceedings of the 25th ACM SIGKDD International\\nConference on Knowledge Discovery &amp; DataICAART 2023 - 15th International Conference on Agents and Artiﬁcial Intelligence\\n688', metadata={'source': '/content/Hallucinations.pdf', 'page': 6}),\n",
              " Document(page_content='Mining , KDD ’19, page 166–175, New York, NY ,\\nUSA. Association for Computing Machinery.\\nGunel, B., Zhu, C., Zeng, M., and Huang, X. (2019). Mind\\nthe facts: Knowledge-boosted coherent abstractive\\ntext summarization. NeurIPS, Knowledge Representa-\\ntion & Reasoning Meets Machine Learning (KR2ML\\nworkshop) , abs/2006.15435.\\nGupta, S. and Gupta, S. K. (2019). Abstractive summa-\\nrization: An overview of the state of the art. Expert\\nSystems with Applications , 121:49–65.\\nHochreiter, S., Schmidhuber, J., and Elvezia, C. (1997).\\nLong short-term memory. Neural Computation ,\\n9(8):1735–1780.\\nHuang, Y ., Feng, X., Feng, X., and Qin, B. (2021). The\\nfactual inconsistency problem in abstractive text sum-\\nmarization: A survey. CoRR , abs/2104.14839.\\nKarouzos, C., Paraskevopoulos, G., and Potamianos, A.\\n(2021). UDALM: Unsupervised domain adaptation\\nthrough language modeling. In Proceedings of the\\n2021 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies , pages 2579–2590, On-\\nline. Association for Computational Linguistics.\\nKitaev, N., Kaiser, L., and Levskaya, A. (2020). Reformer:\\nThe efﬁcient transformer. CoRR , abs/2001.04451.\\nKlymenko, O., Braun, D., and Matthes, F. (2020). Auto-\\nmatic text summarization: A state-of-the-art review.\\nICEIS (1) , pages 648–655.\\nKotonya, N. and Toni, F. (2020). Explainable automated\\nfact-checking for public health claims. In Proceed-\\nings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing (EMNLP) , Online. As-\\nsociation for Computational Linguistics.\\nKryscinski, W., McCann, B., Xiong, C., and Socher, R.\\n(2020). Evaluating the factual consistency of abstrac-\\ntive text summarization. In Proceedings of the 2020\\nConference on Empirical Methods in Natural Lan-\\nguage Processing (EMNLP) , pages 9332–9346, On-\\nline. Association for Computational Linguistics.\\nKudo, T. and Richardson, J. (2018). Sentencepiece: A\\nsimple and language independent subword tokenizer\\nand detokenizer for neural text processing. CoRR ,\\nabs/1808.06226.\\nLewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo-\\nhamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer,\\nL. (2019). BART: denoising sequence-to-sequence\\npre-training for natural language generation, transla-\\ntion, and comprehension. CoRR , abs/1910.13461.\\nLin, C.-Y . (2004). ROUGE: A package for automatic evalu-\\nation of summaries. In Text Summarization Branches\\nOut, pages 74–81, Barcelona, Spain. Association for\\nComputational Linguistics.\\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov,\\nV . (2019). Roberta: A robustly optimized bert pre-\\ntraining approach.\\nMaynez, J., Narayan, S., Bohnet, B., and McDonald, R.\\n(2020). On faithfulness and factuality in abstractive\\nsummarization. In Proceedings of the 58th AnnualMeeting of the Association for Computational Lin-\\nguistics , pages 1906–1919, Online. Association for\\nComputational Linguistics.\\nMoradi, M., Dashti, M., and Samwald, M. (2020). Summa-\\nrization of biomedical articles using domain-speciﬁc\\nword embeddings and graph ranking. Journal of\\nBiomedical Informatics , 107:103452.\\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).\\nBleu: A method for automatic evaluation of ma-\\nchine translation. In Proceedings of the 40th Annual\\nMeeting on Association for Computational Linguis-\\ntics, ACL ’02, page 311–318, USA. Association for\\nComputational Linguistics.\\nSachidananda, V ., Kessler, J., and Lai, Y .-A. (2021). Efﬁ-\\ncient domain adaptation of language models via adap-\\ntive tokenization. Proceedings of the Second Work-\\nshop on Simple and Efﬁcient Natural Language Pro-\\ncessing , pages 155–165.\\nSai, A. B., Mohankumar, A. K., and Khapra, M. M. (2022).\\nA survey of evaluation metrics used for nlg systems.\\nACM Computing Surveys (CSUR) , 55(2):1–39.\\nSong, K., Lebanoff, L., Guo, Q., Qiu, X., Xue, X., Li,\\nC., Yu, D., and Liu, F. (2020a). Joint parsing and\\ngeneration for abstractive summarization. Proceed-', metadata={'source': '/content/Hallucinations.pdf', 'page': 7}),\n",
              " Document(page_content='ACM Computing Surveys (CSUR) , 55(2):1–39.\\nSong, K., Lebanoff, L., Guo, Q., Qiu, X., Xue, X., Li,\\nC., Yu, D., and Liu, F. (2020a). Joint parsing and\\ngeneration for abstractive summarization. Proceed-\\nings of the AAAI Conference on Artiﬁcial Intelligence ,\\n34(05):8894–8901.\\nSong, X., Salcianu, A., Song, Y ., Dopson, D., and Zhou, D.\\n(2020b). Linear-time wordpiece tokenization. CoRR ,\\nabs/2012.15524.\\nTay, Y ., Dehghani, M., Bahri, D., and Metzler, D.\\n(2020). Efﬁcient transformers: A survey. CoRR ,\\nabs/2009.06732.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I.\\n(2017). Attention is all you need. Proceedings of the\\n31st International Conference on Neural Information\\nProcessing Systems , pages 6000–6010.\\nZaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Al-\\nberti, C., Onta ˜n´on, S., Pham, P., Ravula, A., Wang,\\nQ., Yang, L., and Ahmed, A. (2020). Big bird: Trans-\\nformers for longer sequences. CoRR , abs/2007.14062.\\nZeng, X., Abumansour, A. S., and Zubiaga, A. (2021). Au-\\ntomated fact-checking: A survey. Language and Lin-\\nguistics Compass , 15(10):e12438.\\nZhang, J., Zhao, Y ., Saleh, M., and Liu, P. J. (2019). PE-\\nGASUS: pre-training with extracted gap-sentences for\\nabstractive summarization. CoRR , abs/1912.08777.\\nZhang, T., Kishore, V ., Wu, F., Weinberger, K. Q., and\\nArtzi, Y . (2020). Bertscore: Evaluating text genera-\\ntion with BERT. In 8th International Conference on\\nLearning Representations, ICLR 2020, Addis Ababa,\\nEthiopia, April 26-30, 2020 . OpenReview.net.\\nZhong, J., Wang, Z., and Li, Q. (2022). Mtl-das: Auto-\\nmatic text summarization for domain adaptation. In-\\ntell. Neuroscience , 2022.\\nZhou, C., Neubig, G., Gu, J., Diab, M., Guzm ´an, F., Zettle-\\nmoyer, L., and Ghazvininejad, M. (2021). Detecting\\nhallucinated content in conditional neural sequence\\ngeneration. In Findings of the Association for Com-\\nputational Linguistics: ACL-IJCNLP 2021 .Challenges in Domain-Speciﬁc Abstractive Summarization and How to Overcome Them\\n689', metadata={'source': '/content/Hallucinations.pdf', 'page': 7})]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZjjKyi1m_lA"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV04kCtEm_tF",
        "outputId": "60530442-33bd-4189-92f4-d7e2675dc65e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='Challenges in Domain-Speciﬁc Abstractive Summarization and How to\\nOvercome Them\\nAnum Afzal1,\n",
            "Juraj Vladika1, Daniel Braun2and Florian Matthes1\\n1Department of Computer Science, Technical University of\n",
            "Munich, Boltzmannstrasse 3, 85748\\nGarching bei Muenchen, Germany\\n2Department of High-tech Business and\n",
            "Entrepreneurship, University of Twente, Hallenweg 17,\\n \\nKeywords: Text Summarization, Natural Language\n",
            "Processing, Efﬁcient Transformers, Model Hallucination, Natural\\nLanguage Generation Evaluation, Domain-\n",
            "Adaptation of Language Models.\\nAbstract: Large Language Models work quite well with general-purpose data and\n",
            "many tasks in Natural Language\\nProcessing. However, they show several limitations when used for a task such\n",
            "as domain-speciﬁc abstractive\\ntext summarization. This paper identiﬁes three of those limitations as research\n",
            "problems in the context of\\nabstractive text summarization: 1) Quadratic complexity of transformer-based\n",
            "models with respect to the\\ninput text length; 2) Model Hallucination, which is a model’s ability to generate\n",
            "factually incorrect text; and\\n3) Domain Shift, which happens when the distribution of the model’s training\n",
            "and test corpus is not the same.\\nAlong with a discussion of the open research questions, this paper also\n",
            "provides an assessment of existing\\nstate-of-the-art techniques relevant to domain-speciﬁc text summarization\n",
            "to address the research gaps.\\n1 INTRODUCTION\\nWith the ever-increasing amount of textual data be-\\ning\n",
            "created, stored, and digitized, companies and re-\\nsearchers have large corpora at their disposal that\\ncould\n",
            "be processed into useful information. Perusal\\nand encapsulation of such data usually require\n",
            "domain\\nexpertise which is costly and time-consuming. Ab-\\nstractive text summarization using Natural\n",
            "Language\\nProcessing (NLP) techniques, is a powerful tool that\\ncan provide aid for this task. Unlike the\n",
            "traditional\\nautomatic text summarization techniques, which ex-\\ntracts the most relevant sentences from the\n",
            "original\\ndocument, abstractive text summarization generates\\nnew text as summaries. For the sake of\n",
            "simplicity, the\\nterm text summarization would be used to represent\\nabstractive text summarization in this\n",
            "paper.\\nWhile text summarization (Gupta and Gupta,\\n2019; Klymenko et al., 2020) on general textual data\\nhas\n",
            "been an active research ﬁeld in the past decade,\\nsummarization of domain-speciﬁc documents, espe-\\ncially to\n",
            "support business and scientiﬁc processes\\nhave not received much attention. State-of-the-art\\nresearch focuses\n",
            "on deep learning models in NLP\\nto capture semantics and context associated with the\\ntext. While these Large\n",
            "Language Models (LLMs)perform well on the general-purpose corpus, their\\nperformance declines when tested\n",
            "against domain-\\nspeciﬁc corpus. This paper discusses some challenges\\nLLMs face in the context of a text\n",
            "summarization task\\nand provides an overview of existing techniques that\\ncould be leveraged to counter those\n",
            "challenges.\\nPrevious research in text summarization has\\nmostly focused on general-purpose data (Gupta\n",
            "and\\nGupta, 2019; Allahyari et al., 2017). Domain-speciﬁc\\nsummarization however, is still an active research\n",
            "area\\nand has many research questions that need to be ad-\\ndressed. This paper addresses some of those\n",
            "theo-\\nretical research questions and provides an initial as-\\nsessment of the existing techniques can be\n",
            "utilized\\nto overcome those limitations. We have identiﬁed\\nthree important hurdles with regards to the\n",
            "success-\\nful implementation of an NLP model for generation\\nof domain-speciﬁc summaries.\\n1. Most language\n",
            "models have quadratic complex-\\nity, meaning that the memory requirement grows\\nquadratically as the length of\n",
            "the text increases.\\nAs a result, transformer-based language models\\nare not capable of processing large\n",
            "documents.\\nSince most documents that need to be summarized\\nare long, it creates a need for language models\n",
            "ca-\\npable of handling them efﬁciently without over-682\\nAfzal, A., Vladika, J., Braun, D. and Matthes, F .'\n",
            "metadata={'source': '/content/Hallucinations.pdf', 'page': 0}\n"
          ]
        }
      ],
      "source": [
        "print(wrap_text_preserve_newlines(str(documents[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLO2H5x7muua"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qXNpk8vXlys"
      },
      "outputs": [],
      "source": [
        "# Text Splitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEUp4OtxnuXL",
        "outputId": "b737a52e-2e2d-4a64-a313-a4987d63b482"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_HUKypknucr",
        "outputId": "c6046b5b-7d44-455e-b4da-9df5320e34e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='Challenges in Domain-Speciﬁc Abstractive Summarization and How to\\nOvercome Them\\nAnum Afzal1, Juraj Vladika1, Daniel Braun2and Florian Matthes1\\n1Department of Computer Science, Technical University of Munich, Boltzmannstrasse 3, 85748\\nGarching bei Muenchen, Germany\\n2Department of High-tech Business and Entrepreneurship, University of Twente, Hallenweg 17,\\n \\nKeywords: Text Summarization, Natural Language Processing, Efﬁcient Transformers, Model Hallucination, Natural\\nLanguage Generation Evaluation, Domain-Adaptation of Language Models.\\nAbstract: Large Language Models work quite well with general-purpose data and many tasks in Natural Language\\nProcessing. However, they show several limitations when used for a task such as domain-speciﬁc abstractive\\ntext summarization. This paper identiﬁes three of those limitations as research problems in the context of\\nabstractive text summarization: 1) Quadratic complexity of transformer-based models with respect to the\\ninput text length; 2) Model Hallucination, which is a model’s ability to generate factually incorrect text; and\\n3) Domain Shift, which happens when the distribution of the model’s training and test corpus is not the same.\\nAlong with a discussion of the open research questions, this paper also provides an assessment of existing\\nstate-of-the-art techniques relevant to domain-speciﬁc text summarization to address the research gaps.\\n1 INTRODUCTION\\nWith the ever-increasing amount of textual data be-\\ning created, stored, and digitized, companies and re-\\nsearchers have large corpora at their disposal that\\ncould be processed into useful information. Perusal\\nand encapsulation of such data usually require domain\\nexpertise which is costly and time-consuming. Ab-\\nstractive text summarization using Natural Language\\nProcessing (NLP) techniques, is a powerful tool that\\ncan provide aid for this task. Unlike the traditional\\nautomatic text summarization techniques, which ex-\\ntracts the most relevant sentences from the original\\ndocument, abstractive text summarization generates\\nnew text as summaries. For the sake of simplicity, the\\nterm text summarization would be used to represent\\nabstractive text summarization in this paper.\\nWhile text summarization (Gupta and Gupta,\\n2019; Klymenko et al., 2020) on general textual data\\nhas been an active research ﬁeld in the past decade,\\nsummarization of domain-speciﬁc documents, espe-\\ncially to support business and scientiﬁc processes\\nhave not received much attention. State-of-the-art\\nresearch focuses on deep learning models in NLP\\nto capture semantics and context associated with the\\ntext. While these Large Language Models (LLMs)perform well on the general-purpose corpus, their\\nperformance declines when tested against domain-\\nspeciﬁc corpus. This paper discusses some challenges\\nLLMs face in the context of a text summarization task\\nand provides an overview of existing techniques that\\ncould be leveraged to counter those challenges.\\nPrevious research in text summarization has\\nmostly focused on general-purpose data (Gupta and\\nGupta, 2019; Allahyari et al., 2017). Domain-speciﬁc\\nsummarization however, is still an active research area\\nand has many research questions that need to be ad-\\ndressed. This paper addresses some of those theo-\\nretical research questions and provides an initial as-\\nsessment of the existing techniques can be utilized\\nto overcome those limitations. We have identiﬁed\\nthree important hurdles with regards to the success-\\nful implementation of an NLP model for generation\\nof domain-speciﬁc summaries.\\n1. Most language models have quadratic complex-\\nity, meaning that the memory requirement grows\\nquadratically as the length of the text increases.\\nAs a result, transformer-based language models\\nare not capable of processing large documents.\\nSince most documents that need to be summarized\\nare long, it creates a need for language models ca-\\npable of handling them efﬁciently without over-682\\nAfzal, A., Vladika, J., Braun, D. and Matthes, F .', metadata={'source': '/content/Hallucinations.pdf', 'page': 0})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkfe4cA3aQqi",
        "outputId": "7b2b2f80-2a59-4f91-feaa-6e3c1ef533ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='are long, it creates a need for language models ca-\\npable of handling them efﬁciently without over-682\\nAfzal, A., Vladika, J., Braun, D. and Matthes, F .\\nChallenges in Domain-Speciﬁc Abstractive Summarization and How to Overcome Them.\\nDOI: 10.5220/0011744500003393\\nInProceedings of the 15th International Conference on Agents and Artiﬁcial Intelligence (ICAART 2023) - Volume 3 , pages 682-689\\nISBN: 978-989-758-623-1; ISSN: 2184-433X\\nCopyright c⃝2023 by SCITEPRESS – Science and Technology Publications, Lda. Under CC license (CC BY -NC-ND 4.0)', metadata={'source': '/content/Hallucinations.pdf', 'page': 0})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6HWzhWUaRIb"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uctYbK6YXmxI"
      },
      "outputs": [],
      "source": [
        "# Embeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSK5JGBmBejJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wS3_6gbXqvE",
        "outputId": "b248be09-201c-4b7f-d393-f935aa2fb412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "264Et_-RXuv4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTg8T2tJXvTI"
      },
      "outputs": [],
      "source": [
        "# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYQOej1MpiH2"
      },
      "outputs": [],
      "source": [
        "query = \"What is hallucination?\"\n",
        "docs = db.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7li6KoVpiQh",
        "outputId": "0b3d6638-a5be-4039-a598-26b7c996c819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "one of them being abstractive summarization. The\n",
            "output of models for NLG tasks is notoriously hard\n",
            "to evaluate because there is usually a trade-off be-\n",
            "tween the expressiveness of the model and its fac-\n",
            "tual accuracy (Sai et al., 2022). Metrics to evaluate\n",
            "generated text can be word-based, character-based, or\n",
            "embedding-based. Word-based metrics are the most\n",
            "popular evaluation metrics, owing to their ease of use.\n",
            "They look at the exact overlap of n-grams (n consec-\n",
            "utive words) between generated and reference text.\n",
            "Their main drawback is that they do not take into ac-\n",
            "count the meaning of the text. Two sentences such\n",
            "as “Berlin is the capital of Germany ” and “ Berlin is\n",
            "not the capital of Germany ” have an almost complete\n",
            "n-gram overlap despite having opposite meanings.\n",
            "2.2.2 Model Hallucinations\n",
            "Even though modern transformer models can gener-\n",
            "ate text that is coherent and grammatically correct,\n",
            "they are prone to generating content not backed by the\n",
            "source document. Borrowing the terminology from\n",
            "psychology, this is called model hallucination. In ab-\n",
            "stractive summarization, the summary is said to be\n",
            "hallucinated if it has any spans not supported by con-\n",
            "tent in the input document (Maynez et al., 2020). Two\n",
            "main types of hallucinations are (1) intrinsic, where\n",
            "the generated content contradicts the source docu-\n",
            "ment; and (2) extrinsic, which are facts that cannot be\n",
            "veriﬁed from the source document. For example, if\n",
            "the document mentions an earthquake that happened\n",
            "in 2020, an intrinsic hallucination is saying it hap-\n",
            "pened in 2015, while an extrinsic one would be a sen-\n",
            "tence about a ﬂood that is never even mentioned in\n",
            "the document. In their analysis of three recent state-\n",
            "of-the-art abstractive summarization systems, (Falke\n",
            "et al., 2019) show that 25% of generated summaries\n",
            "contain hallucinated content. Hallucinations usually\n",
            "stem from pre-trained large models introducing facts\n",
            "they learned during their training process, which is\n",
            "unrelated to a given source document.\n",
            "2.3 Domain Shift in Natural Language\n",
            "Processing\n",
            "When working with speciﬁc NLP applications, do-\n",
            "main knowledge is paramount for success. Finding\n",
            "labeled training data, or even unlabeled data in some\n",
            "cases, is a big challenge. Training data is often scarce\n",
            "in many domains/languages and often hinders the so-\n",
            "lution development for domain-speciﬁc tasks in NLP.\n",
            "Transfer Learning provides a solution to this by uti-\n",
            "lizing the existing model knowledge and building on\n",
            "it when training the model for a new task. Essentially,it allows the transfer and adaptation of the knowledge\n",
            "acquired from one set of domains and tasks to another\n",
            "set of domains and tasks.\n",
            "Transformer-based language models in tandem\n",
            "with Transfer Learning have proven to be quite suc-\n",
            "cessful in the past years and have found their appli-\n",
            "cation in several real-world use cases. While they\n",
            "work well with tasks involving general-purpose cor-\n",
            "pus, there is a performance decline when it comes\n",
            "to domain-speciﬁc data. This happens because these\n",
            "language models are pre-trained on general-purpose\n",
            "data but are then tested on a domain-speciﬁc corpus.\n",
            "This difference in the distribution of training and test-\n",
            "ing data is known as the Domain Shift problem in\n",
            "NLP. It essentially means that the model doesn’t know\n",
            "the domain-speciﬁc corpus contains words and con-\n",
            "cepts since they were not part of model’s pre-training.\n",
            "3 EXISTING TECHNIQUES\n",
            "This section presents an overview of the existing tech-\n",
            "niques and architectures that can be applied for the\n",
            "summarization of domain-speciﬁc documents. These\n",
            "techniques are categorized into three sections based\n",
            "on the research questions they address; Efﬁcient\n",
            "Transformers, Evaluation metrics, and Domain adap-\n",
            "tation of Language Models. These techniques are\n",
            "summarized in Table 1, and discussed in detail in the\n",
            "section below.\n",
            "3.1 Efﬁcient Transformers\n",
            "The quadratic complexity of the Transformer block\n",
            "is a well-known issue and several approaches to\n"
          ]
        }
      ],
      "source": [
        "print(wrap_text_preserve_newlines(str(docs[0].page_content)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsSGNm8UdsX3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzWUbpH6bQ2l"
      },
      "source": [
        "### Create QA Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzJnHsJ_bThf"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import HuggingFaceHub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IZn8b8Aci4w"
      },
      "outputs": [],
      "source": [
        "llm=HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-v0.1\", model_kwargs={\"temperature\":0.7})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2IMgDRGsVV4"
      },
      "outputs": [],
      "source": [
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aemehFTEsVZG",
        "outputId": "912b5806-b8ae-4804-8b67-d1962b076c84"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Hallucination is a phenomenon\\ncharacterized by sensing things that are not present\\nin reality'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"What is hallucination?\"\n",
        "docs = db.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SVOXzjaNsVb8",
        "outputId": "6337cbbd-8eec-4711-eb02-bdb8e430f0d0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' BigBird is an efﬁcient Transformer\\nmodel for long sequences\\nHall'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"what is BigBird?\"\n",
        "docs = db.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNdVzDaNe39j"
      },
      "source": [
        "### Working with PDF Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0NDARXbveTcI",
        "outputId": "8b94b178-826f-48d3-884e-08a0a5f127cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting unstructured\n",
            "  Downloading unstructured-0.10.18-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.11.2)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.1)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2023.6.15-py3-none-any.whl (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.1/275.1 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.23.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2023.7.22)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (4.5.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=37b048739f0613d1581b3794d7bb6ea02deab4818313e047679b78433c72f846\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, python-magic, python-iso639, langdetect, emoji, unstructured\n",
            "Successfully installed emoji-2.8.0 filetype-1.2.0 langdetect-1.0.9 python-iso639-2023.6.15 python-magic-0.4.27 unstructured-0.10.18\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.13-py3-none-any.whl (437 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.8/437.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.13)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.103.2-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.14.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (2.0.5)\n",
            "Requirement already satisfied: huggingface_hub<0.17,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.16.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=73a8cbe2d789a8b6079e6219a74ea88a15d2fef4864971c1cb37fb17a842f3df\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, websockets, uvloop, python-dotenv, pulsar-client, overrides, humanfriendly, httptools, h11, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, starlette, posthog, coloredlogs, onnxruntime, fastapi, chromadb\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.0.1 chroma-hnswlib-0.7.3 chromadb-0.4.13 coloredlogs-15.0.1 fastapi-0.103.2 h11-0.14.0 httptools-0.6.0 humanfriendly-10.0 monotonic-1.6 onnxruntime-1.16.0 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.20.0 websockets-11.0.3\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.5.1\n",
            "Requirement already satisfied: unstructured[local-inference] in /usr/local/lib/python3.10/dist-packages (0.10.18)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (4.9.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (4.11.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (2.8.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (0.6.1)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (2023.6.15)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (1.23.5)\n",
            "Collecting unstructured-inference==0.5.31 (from unstructured[local-inference])\n",
            "  Downloading unstructured_inference-0.5.31-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msg-parser (from unstructured[local-inference])\n",
            "  Downloading msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypandoc (from unstructured[local-inference])\n",
            "  Downloading pypandoc-1.11-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (3.4.4)\n",
            "Collecting pdfminer.six (from unstructured[local-inference])\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (3.1.2)\n",
            "Collecting python-pptx<=0.6.21 (from unstructured[local-inference])\n",
            "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (2.0.1)\n",
            "Collecting pdf2image (from unstructured[local-inference])\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Collecting python-docx (from unstructured[local-inference])\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[local-inference])\n",
            "  Downloading unstructured.pytesseract-0.3.12-py3-none-any.whl (14 kB)\n",
            "Collecting ebooklib (from unstructured[local-inference])\n",
            "  Downloading EbookLib-0.18.tar.gz (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (1.5.3)\n",
            "Collecting layoutparser[layoutmodels,tesseract] (from unstructured-inference==0.5.31->unstructured[local-inference])\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart (from unstructured-inference==0.5.31->unstructured[local-inference])\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.5.31->unstructured[local-inference]) (0.16.4)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.5.31->unstructured[local-inference]) (4.8.0.76)\n",
            "Collecting onnx==1.14.1 (from unstructured-inference==0.5.31->unstructured[local-inference])\n",
            "  Downloading onnx-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.5.31->unstructured[local-inference]) (1.16.0)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.5.31->unstructured[local-inference]) (4.34.0)\n",
            "Collecting rapidfuzz (from unstructured-inference==0.5.31->unstructured[local-inference])\n",
            "  Downloading rapidfuzz-3.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx==1.14.1->unstructured-inference==0.5.31->unstructured[local-inference]) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx==1.14.1->unstructured-inference==0.5.31->unstructured[local-inference]) (4.5.0)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx<=0.6.21->unstructured[local-inference]) (9.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<=0.6.21->unstructured[local-inference])\n",
            "  Downloading XlsxWriter-3.1.6-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.3/154.3 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[local-inference]) (23.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[local-inference]) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[local-inference]) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[local-inference]) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ebooklib->unstructured[local-inference]) (1.16.0)\n",
            "Collecting olefile>=0.46 (from msg-parser->unstructured[local-inference])\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[local-inference]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[local-inference]) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[local-inference]) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[local-inference]) (4.66.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured[local-inference]) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[local-inference]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[local-inference]) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[local-inference]) (3.2.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[local-inference]) (41.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[local-inference]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[local-inference]) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[local-inference]) (2023.7.22)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[local-inference]) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.5.31->unstructured[local-inference]) (3.12.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.5.31->unstructured[local-inference]) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.5.31->unstructured[local-inference]) (0.14.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.5.31->unstructured[local-inference]) (0.3.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.5.31->unstructured[local-inference]) (2023.6.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[local-inference]) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (1.11.3)\n",
            "Collecting iopath (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference])\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference])\n",
            "  Downloading pdfplumber-0.10.2-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytesseract (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference])\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (0.15.2+cu118)\n",
            "Collecting effdet (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference])\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime->unstructured-inference==0.5.31->unstructured[local-inference]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->unstructured-inference==0.5.31->unstructured[local-inference]) (23.5.26)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime->unstructured-inference==0.5.31->unstructured[local-inference]) (1.12)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[local-inference]) (2.21)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime->unstructured-inference==0.5.31->unstructured[local-inference]) (10.0)\n",
            "Collecting timm>=0.9.2 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference])\n",
            "  Downloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (2.0.7)\n",
            "Collecting omegaconf>=2.0 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference])\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (3.27.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (17.0.1)\n",
            "Collecting portalocker (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference])\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference])\n",
            "  Downloading pypdfium2-4.20.0-py3-none-manylinux_2_17_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime->unstructured-inference==0.5.31->unstructured[local-inference]) (1.3.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.5.31->unstructured[local-inference]) (3.1.1)\n",
            "Building wheels for collected packages: python-pptx, ebooklib, python-docx, olefile, iopath, antlr4-python3-runtime\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470932 sha256=45fd7179959075473ca65dd4432a39d7eeadcc0236c3e1529fbca7f0a18b8c0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/dd/74/01b3ec7256a0800b99384e9a0f7620e358afc3a51a59bf9b49\n",
            "  Building wheel for ebooklib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ebooklib: filename=EbookLib-0.18-py3-none-any.whl size=38778 sha256=41fd26b9574d7390fecefd1d0b16fb0dced6cfcd46985a51372bb182d1e39210\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/38/cc/a3728bb72a315d9d8766fb71d362136372066fc25ad838f8fa\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184487 sha256=ed27788105b97ab94cd5b36d31f96f5cfcb9e6e5d973abd934d3148f3b7dd904\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=ebcc7b3d3b90d8d5a9b9c92efc1edd581267000436e0fe26db5c38b9f341f489\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/39/c0/9eb1f7a42b4b38f6f333b6314d4ed11c46f12a0f7b78194f0d\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31530 sha256=3bbec633a960b9f4840c56346e44990625d8078907e61f4d1a98c9c96c0bdbe4\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=21baa52e9eabf148a237ff3a35d83071de10872ec3c564bc1257abcda27428bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built python-pptx ebooklib python-docx olefile iopath antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, XlsxWriter, unstructured.pytesseract, rapidfuzz, python-multipart, python-docx, pytesseract, pypdfium2, pypandoc, portalocker, pdf2image, onnx, omegaconf, olefile, ebooklib, python-pptx, msg-parser, iopath, pdfminer.six, pdfplumber, layoutparser, timm, effdet, unstructured-inference\n",
            "Successfully installed XlsxWriter-3.1.6 antlr4-python3-runtime-4.9.3 ebooklib-0.18 effdet-0.4.1 iopath-0.1.10 layoutparser-0.3.4 msg-parser-1.2.0 olefile-0.46 omegaconf-2.3.0 onnx-1.14.1 pdf2image-1.16.3 pdfminer.six-20221105 pdfplumber-0.10.2 portalocker-2.8.2 pypandoc-1.11 pypdfium2-4.20.0 pytesseract-0.3.10 python-docx-0.8.11 python-multipart-0.0.6 python-pptx-0.6.21 rapidfuzz-3.3.1 timm-0.9.7 unstructured-inference-0.5.31 unstructured.pytesseract-0.3.12\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install unstructured\n",
        "!pip install chromadb\n",
        "!pip install Cython\n",
        "!pip install tiktoken\n",
        "!pip install unstructured[local-inference]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juVVrXsUfybx"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z1YygkktO12",
        "outputId": "a22cac2c-674a-4113-9b6b-5f41f69a10d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['2023_GPT4All_Technical_Report.pdf', '2008.10010.pdf']"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# connect your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "pdf_folder_path = '/content/gdrive/My Drive/data_2/'\n",
        "os.listdir(pdf_folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGbsRBujILnj"
      },
      "outputs": [],
      "source": [
        "pdf_folder_path ='/content/RE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5lRcsUWtO42",
        "outputId": "72fa2359-eb0f-41e0-e1b1-40d7125b585d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<langchain.document_loaders.pdf.UnstructuredPDFLoader at 0x7d07509f87f0>,\n",
              " <langchain.document_loaders.pdf.UnstructuredPDFLoader at 0x7d07509fa920>]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loaders = [UnstructuredPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]\n",
        "loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBsI8myktO8Y",
        "outputId": "c8feabd2-e652-4041-8df4-9735e48c6484"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 3369, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 4006, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 4246, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1525, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2056, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 4547, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 4429, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 4721, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1387, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1025, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1128, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1204, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1086, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1031, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1024, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2098, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1500, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1743, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2494, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1446, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1322, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1602, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1466, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1253, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1732, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1436, which is longer than the specified 1000\n"
          ]
        }
      ],
      "source": [
        "index = VectorstoreIndexCreator(\n",
        "    embedding=HuggingFaceEmbeddings(),\n",
        "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)).from_loaders(loaders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7py0d3o5tO_h"
      },
      "outputs": [],
      "source": [
        "llm=HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-v0.1\", model_kwargs={\"temperature\":0.5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31lq4mOdtPDL"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                    chain_type=\"stuff\",\n",
        "                                    retriever=index.vectorstore.as_retriever(),\n",
        "                                    input_key=\"question\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "19VMjTc_tPKB",
        "outputId": "781eaa82-d479-4641-cc60-9384b7c40288"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' The paper summarizes the effectiveness of summarization models on a dataset of 100k research'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run('Provide a summary of the paper')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "biOFwTy4tQBE",
        "outputId": "7739d6f4-9410-40a8-e9c1-5a3d9029a2e8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' BigBird is a 175 billion parameter language model trained on a corpus of '"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run('What is Bigbird?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HdvEc7MJslk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
